* learning rate: 	$\alpha\in\mathbb{R}$ 
	  ----
* update rule of gradient descent:
	$$\boxed{\theta\longleftarrow\theta-\alpha\nabla J(\theta)}$$  
	  
	---
	![[Pasted image 20201104003420.png]]
	
	---
	>Remark: Stochastic gradient descent (SGD) is updating the parameter based on each training example, and batch gradient descent is on a batch of training examples.
