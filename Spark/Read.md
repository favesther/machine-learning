# from files
```py
# In Python
from pyspark.sql.types import *
inputDirectoryOfJsonFiles =  ... 

fileSchema = (StructType()
  .add(StructField("key", IntegerType()))
  .add(StructField("value", IntegerType())))

inputDF = (spark
  .readStream
  .format("json")
  .schema(fileSchema)
  .load(inputDirectoryOfJsonFiles))
```

# Apache Kafka

```py
inputDF = (spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "events")
  .load())
```

Table 8-1. Schema of the DataFrame generated by the Kafka source

Column name|Column type|Description
:--:|:--:|:--
`key`|`binary`|Key data of the record as bytes.
`value`|`binary`|Value data of the record as bytes.
`topic`|`string`|Kafka topic the record was in. This is useful when subscribed to multiple topics.
`partition`|`int`|Partition of the Kafka topic the record was in.
`offset`|`long`|Offset value of the record.
`timestamp`|`long`|Timestamp associated with the record.
`timestampType`|`int`|Enumeration for the type of the timestamp associated with the record.